Your friend Tom is a big gambler as well as a sports lover. He loves betting with his friends and he had an idea for a new challenge: Betting on which countries will win at least one gold medal at the next Olympics. \\

In order to increase his chances of winning, he wants to build a binary classifier. For each country, he already knows whether it won at least one gold medal at the last Olympics (binary target variable: YES if the country won at least one gold medal and NO if it didn't). Understandably, no classifier will be able to predict very well with little information. Tom therefore came up with a few potential features he could use. Before going ahead with getting data for these potential features, cleaning it (etc.), Tom turns to you. He would like you to rank these potential features based on how much you expect them to increase classifier performance. Using your ranking, Tom will acquire data for your highest ranked feature first, followed by the second, etc. \\

All of Tom's feature ideas are binary, this means that they can either be true or false. Note, that numeric variables can easily be transformed into binary variables by binning them. For example, the variable "unemployment rate" can be binned into "low unemployment rate (<6%)", "medium unemployment rate (6%-9%)" and "high unemployment rate (>9%)" at some information loss. In that case, we treat each bin as an individual feature, since some bins might be more valuable to Richard's classifier than others. Also note that categorical variables can easily be transformed into binary variables through dummy extraction. In this case, we also treat all arising dummies as separate features. \\

Please help Tom and rank the list of potential features by relevancy. The element on the top of the list should promise the largest improvement to the classifier, the second should see the second highest improvement in classifier performance, etc...
